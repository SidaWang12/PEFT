learning_rate: 1e-6
output_dir: logs
per_device_train_batch_size: 32
# per_device_eval_batch_size: 16
gradient_accumulation_steps: 1
logging_steps: 1
max_steps: 5
# save_steps: 5
chat_template: None

dataset_name: lmms-lab/Math10K
model_name_or_path: Qwen/Qwen2.5-1.5B # openlm-research/open_llama_3b #NousResearch/Llama-2-7b-hf
trust_remote_code: true
torch_dtype: bfloat16
bf16: true
use_cpu: false
seed: 42
num_train_epochs: 3
max_length: 1024
use_cache: false

torch_dtype: bfloat16
# attn_implementation: "flash_attention_2"
preprocessing_num_workers: 48
do_eval: true
eval_steps: 10
eval_strategy: "steps"
gradient_checkpointing: true


