enable_analysis: true

learning_rate: 1e-6
output_dir: logs
per_device_train_batch_size: 16
# per_device_eval_batch_size: 16
gradient_accumulation_steps: 1
logging_steps: 1
num_train_epochs: 0.01
# max_steps: 5
# save_steps: 5

dataset_name: lmms-lab/Math10K
model_name_or_path: Qwen/Qwen2.5-1.5B # openlm-research/open_llama_3b #NousResearch/Llama-2-7b-hf
trust_remote_code: true
torch_dtype: bfloat16
bf16: true
use_cpu: false
seed: 42
max_length: 1024
use_cache: false

torch_dtype: bfloat16
# attn_implementation: "flash_attention_2"
do_eval: false
# eval_steps: 10
# eval_strategy: "steps"
gradient_checkpointing: true


