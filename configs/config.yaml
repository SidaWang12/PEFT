enable_analysis: true
downsample_mlp_blocks_ratio: 0.005
downsample_attention_blocks_ratio: 0.006

learning_rate: 1e-6
output_dir: logs
per_device_train_batch_size: 4
# per_device_eval_batch_size: 16
gradient_accumulation_steps: 1
logging_steps: 1
# num_train_epochs: 0.5
max_steps: 1
# save_steps: 5

dataset_name: lmms-lab/Math10K
model_name_or_path: openlm-research/open_llama_3b # Qwen/Qwen2.5-1.5B #NousResearch/Llama-2-7b-hf
trust_remote_code: true
torch_dtype: bfloat16
bf16: true
use_cpu: false
seed: 42
max_length: 1024

torch_dtype: bfloat16
# attn_implementation: "flash_attention_2"
do_eval: true
eval_steps: 5
eval_strategy: "steps"

# gradient_checkpointing doesn't work with SMT.
gradient_checkpointing: false


