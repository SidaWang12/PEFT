learning_rate: 1e-5
output_dir: /tmp
per_device_train_batch_size: 16
gradient_accumulation_steps: 1
logging_steps: 1
max_steps: 10000
# save_steps: 5
chat_template: None

dataset_name: lmms-lab/Math10K
model_name_or_path: openlm-research/open_llama_3b #NousResearch/Llama-2-7b-hf
trust_remote_code: true
torch_dtype: bfloat16
bf16: true
use_cpu: false
seed: 42
num_train_epochs: 3
max_length: 1024
use_cache: false

torch_dtype: bfloat16
attn_implementation: "flash_attention_2"
preprocessing_num_workers: 48
eval_strategy: 'no'
gradient_checkpointing: true


